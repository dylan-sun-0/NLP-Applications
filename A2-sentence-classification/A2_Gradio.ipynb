{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext==0.12.0"
      ],
      "metadata": {
        "id": "OEcR4qhSxpvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "o5q4RmPXyIy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IH4zhT9wxJMT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3.1\n",
        "# The first time you run this will download a 862MB size file to .vector_cache/glove.6B.zip\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\",dim=100) # embedding size = 100"
      ],
      "metadata": {
        "id": "P2_BH2MfyUtn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self, vocab, k1, k2, n1, n2):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors, freeze=True)\n",
        "        embedding_dim = vocab.vectors.shape[1]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n1, kernel_size=(k1, embedding_dim), bias=False)\n",
        "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=n2, kernel_size=(k2, embedding_dim), bias=False)\n",
        "\n",
        "        self.fc = nn.Linear(n1 + n2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        # need to reorder the layers or else the model breaks, this debug was done by copilot\n",
        "        embedded = embedded.permute(1, 0, 2).unsqueeze(1)\n",
        "\n",
        "        # squeezes at the end  was a google collab auto suggestion (did not work prior but ths fixed it)\n",
        "        conv1_out = F.relu(self.conv1(embedded)).squeeze(3)\n",
        "        conv1_out = F.max_pool1d(conv1_out, conv1_out.shape[2]).squeeze(2)\n",
        "\n",
        "        conv2_out = F.relu(self.conv2(embedded)).squeeze(3)\n",
        "        conv2_out = F.max_pool1d(conv2_out, conv2_out.shape[2]).squeeze(2)\n",
        "\n",
        "        concat_out = torch.cat((conv1_out, conv2_out), dim=1)\n",
        "\n",
        "        output = self.fc(concat_out).squeeze(1)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "UIdpi-eqzzJm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(torch.nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.fc = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embedding(x)\n",
        "        avg_embeddings = torch.mean(embeddings, dim=0)\n",
        "        output = self.fc(avg_embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-WBlZqUIz-Dz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_baseline = torch.load('model_baseline.pt')\n",
        "model_baseline = BaselineModel(glove)\n",
        "model_baseline.load_state_dict(checkpoint_baseline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aguflcdy7wC",
        "outputId": "19e97160-07f0-47a0-f2e0-ce5e9ed3ea1f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_cnn = torch.load('CNN.pt')\n",
        "model_cnn = CNN(glove, k1 = 2, n1 = 20, k2 =3, n2 = 25)\n",
        "model_cnn.load_state_dict(checkpoint_cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjF1ZJpm0USU",
        "outputId": "7c5ee8d6-6f3d-4cb8-9795-19f939b6e0d3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_baseline.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhiajipQ0iT-",
        "outputId": "05e3aa7a-2f6f-49bf-aef6-19355c526bc0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaselineModel(\n",
              "  (embedding): Embedding(400000, 100)\n",
              "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4EOcaY70kd1",
        "outputId": "b9c07596-2162-47ff-8d3d-bd034af09fdd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embedding): Embedding(400000, 100)\n",
              "  (conv1): Conv2d(1, 20, kernel_size=(2, 100), stride=(1, 1), bias=False)\n",
              "  (conv2): Conv2d(1, 25, kernel_size=(3, 100), stride=(1, 1), bias=False)\n",
              "  (fc): Linear(in_features=45, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence):\n",
        "    tokens = sentence.split()\n",
        "    # Convert to integer representation per token\n",
        "    token_ints = [glove.stoi.get(tok, len(glove.stoi)-1) for tok in tokens]\n",
        "    # Convert into a tensor of the shape accepted by the models\n",
        "    token_tensor = torch.LongTensor(token_ints).view(-1,1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        baseline_output = model_baseline(token_tensor)\n",
        "        cnn_output = model_cnn(token_tensor)\n",
        "\n",
        "    # Get probabilities\n",
        "    baseline_prob = torch.sigmoid(baseline_output)\n",
        "    cnn_prob = torch.sigmoid(cnn_output)\n",
        "\n",
        "    # below here google collab auto generated the rest of it\n",
        "    baseline_class = \"Subjective\" if baseline_prob > 0.5 else \"Objective\"\n",
        "    cnn_class = \"Subjective\" if cnn_prob > 0.5 else \"Objective\"\n",
        "\n",
        "    return baseline_class, baseline_prob.item(), cnn_class, cnn_prob.item()\n",
        "\n",
        "# Define the Gradio interface (I used generative AI to help fill out the syntax here, wasn't too sure how to)\n",
        "interface = gr.Interface(\n",
        "    fn=classify_sentence,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter a sentence here...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Baseline Model Classification\"),\n",
        "        gr.Number(label=\"Baseline Model Probability\"),\n",
        "        gr.Textbox(label=\"CNN Model Classification\"),\n",
        "        gr.Number(label=\"CNN Model Probability\")\n",
        "    ],\n",
        "    title=\"Sentence Classification\",\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "D9nLyJX90yR1",
        "outputId": "eb388400-7893-4eab-9fc1-7c09a80dbd64"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://d08df008f8012ad585.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d08df008f8012ad585.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7871 <> https://d08df008f8012ad585.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    }
  ]
}